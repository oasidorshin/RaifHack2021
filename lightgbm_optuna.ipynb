{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6c3cf-3deb-49af-b47e-b2fef852a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data import preprocess_data, postprocessing\n",
    "from func import deviation_metric, get_timestamp\n",
    "\n",
    "from lightgbm import LGBMRegressor, Booster\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae2e55-c4b4-49cd-9c8f-b5b6f1fc3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_search = False\n",
    "training = False\n",
    "inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239b8ef-fd5f-44ee-96db-61dd84456fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom eval metric for CatBoost\n",
    "# Based on https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py\n",
    "def eval_metric(y_true, y_pred):\n",
    "    return \"deviation_metric\", deviation_metric(np.expm1(y_true), np.expm1(y_pred)), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a82a0-13f6-447f-9503-994eb7792070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    eps = trial.suggest_loguniform('eps', 0.01, 0.05) # dbscan param\n",
    "    \n",
    "    data_kwargs = {'cluster': eps,\n",
    "                   'clean_floor_num': True,\n",
    "                   'clean_region_city': True,\n",
    "                   'remove_type_0': True,\n",
    "                   'log_target': True,\n",
    "                   'encode_cat': True}\n",
    "    \n",
    "    train_pre, test_pre, num_columns, cat_columns, target = preprocess_data(train, test, **data_kwargs)\n",
    "    X_columns = num_columns + cat_columns\n",
    "    \n",
    "    # If Kfold\n",
    "    #splitter = StratifiedKFold(n_splits=5)\n",
    "    #splits = splitter.split(train_pre, train_pre[\"realty_type\"]) # Stratify by realty type for stability\n",
    "    \n",
    "    # If holdout set\n",
    "    indices = np.arange(train_pre.shape[0])\n",
    "    splits = [train_test_split(indices, test_size=0.2, random_state=42, stratify = train_pre[\"realty_type\"])]\n",
    "    \n",
    "    lightgbm_params = {'num_leaves': trial.suggest_int('num_leaves', 16, 2048, log = True), \n",
    "                       'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "                       'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 1),\n",
    "                       'n_estimators': 5000,\n",
    "                       'min_child_samples': trial.suggest_int('min_child_samples', 10, 1000, log = True), \n",
    "                       'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-6, 100),\n",
    "                       'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-6, 100),\n",
    "                       'subsample': 0.8,\n",
    "                       'subsample_freq': 1,\n",
    "                       'importance_type': 'gain'}\n",
    "    \n",
    "    eval_scores = []\n",
    "    iters = []\n",
    "    \n",
    "    for train_index, val_index in splits:\n",
    "        train_split = train_pre.iloc[train_index]\n",
    "        val_split = train_pre.iloc[val_index]\n",
    "        \n",
    "        model = LGBMRegressor(**lightgbm_params)\n",
    "        \n",
    "        model.fit(train_split[X_columns], \n",
    "                  train_split[target], \n",
    "                  eval_set = (val_split[X_columns], val_split[target]), \n",
    "                  early_stopping_rounds = 100,\n",
    "                  eval_metric = eval_metric,\n",
    "                  verbose = 500,\n",
    "                  categorical_feature = cat_columns)\n",
    "        \n",
    "        # Return loss, iter\n",
    "        eval_scores.append(model.best_score_[\"valid_0\"][\"deviation_metric\"])\n",
    "        iters.append(model.best_iteration_)\n",
    "        #print(model.best_score_[\"valid_0\"][\"deviation_metric\"], model.best_iteration_)\n",
    "        \n",
    "    trial.set_user_attr(\"es_mean_iter\", np.mean(iters))\n",
    "    return np.mean(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d3a86-8803-47c7-8477-bd3cd7fd64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyper_search:\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548f82c-a875-443c-8e18-93f78a2036c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyper_search:\n",
    "    print(study.best_params)\n",
    "    print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f09bfa-42b4-4df9-8471-9cdb8052de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyper_search:\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "    \n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f8078-c626-4a1c-be39-92bf551582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_kwargs = {'cluster': 0.0271,\n",
    "               'clean_floor_num': True,\n",
    "               'clean_region_city': True,\n",
    "               'remove_type_0': True,\n",
    "               'log_target': True,\n",
    "               'encode_cat': True}\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train_pre, test_pre, num_columns, cat_columns, target = preprocess_data(train, test, **data_kwargs)\n",
    "X_columns = num_columns + cat_columns\n",
    "\n",
    "lightgbm_params = {'num_leaves': 169, \n",
    "                       'max_depth': 5,\n",
    "                       'learning_rate': 0.01416,\n",
    "                       'n_estimators': 5000,\n",
    "                       'min_child_samples': 23, \n",
    "                       'reg_alpha': 1e-6,\n",
    "                       'reg_lambda': 0.000128,\n",
    "                       'subsample': 0.8,\n",
    "                       'subsample_freq': 1,\n",
    "                       'importance_type': 'gain'}\n",
    "\n",
    "if training:\n",
    "    splitter = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "    splits = splitter.split(train_pre, train_pre[\"realty_type\"])\n",
    "    \n",
    "    timestamp = get_timestamp()\n",
    "    \n",
    "    eval_scores = []\n",
    "    feature_importances = []\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(splits):\n",
    "        train_split = train_pre.iloc[train_index]\n",
    "        val_split = train_pre.iloc[val_index]\n",
    "\n",
    "        model = LGBMRegressor(**lightgbm_params)\n",
    "        \n",
    "        model.fit(train_split[X_columns], \n",
    "                  train_split[target], \n",
    "                  eval_set = (val_split[X_columns], val_split[target]), \n",
    "                  early_stopping_rounds = 100,\n",
    "                  eval_metric = eval_metric,\n",
    "                  verbose = 500,\n",
    "                  categorical_feature = cat_columns)\n",
    "\n",
    "        model.booster_.save_model(f\"models/lgbm_fold_{i}_{timestamp}.lgbm\")\n",
    "\n",
    "        # Return loss, iter\n",
    "        eval_scores.append(model.best_score_[\"valid_0\"][\"deviation_metric\"])\n",
    "        feature_importances.append(model.feature_importances_)\n",
    "\n",
    "    print(\"Avg eval score\", np.mean(eval_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947a269-4e26-42a9-b341-5f7ee9eb9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature importances\n",
    "if training:\n",
    "    df = pd.DataFrame(feature_importances, columns = X_columns)\n",
    "    df_mean = df.mean(axis=0).sort_values(ascending = True)\n",
    "    df_mean.plot(kind='barh', figsize=(10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb578d7-1273-4bd2-92c0-b67712028527",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference:\n",
    "    modelnames = [\"lgbm_fold_0_2021-10-11-14-46.lgbm\", \"lgbm_fold_1_2021-10-11-14-46.lgbm\", \"lgbm_fold_2_2021-10-11-14-46.lgbm\", \"lgbm_fold_3_2021-10-11-14-46.lgbm\", \"lgbm_fold_4_2021-10-11-14-46.lgbm\"]\n",
    "    timestamp = get_timestamp()\n",
    "    sample_submission = pd.read_csv('data/test_submission.csv')\n",
    "    predictions = []\n",
    "    \n",
    "    for modelname in modelnames:\n",
    "        model = Booster(model_file=f\"models/{modelname}\")\n",
    "        predictions.append(np.expm1(model.predict(test_pre[X_columns])))\n",
    "        \n",
    "    sample_submission[target] = np.median(np.array(predictions), axis = 0) * 0.94\n",
    "    sample_submission.to_csv(f'submissions/lgbm_{timestamp}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8541537-b7cd-4273-aee4-7f8be389402c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
